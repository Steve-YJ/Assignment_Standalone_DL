{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab7. Basic CNN.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyO4NzEd64UwpsfIL0UZ77HJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c38fc0d3621d42edb002754424cd244f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8406ddf5dcc3439b91d371bc8f79dbd4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_bf3d7c4abe1e426ba0b85ab7cb3a7d04",
              "IPY_MODEL_063c1529d6f2446c9d3d1af9c55b0c9e"
            ]
          }
        },
        "8406ddf5dcc3439b91d371bc8f79dbd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bf3d7c4abe1e426ba0b85ab7cb3a7d04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0606fcb7ac384b1cb5054b5389775d9c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9ed7ce916d334d7aa427fae1c016d0ac"
          }
        },
        "063c1529d6f2446c9d3d1af9c55b0c9e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_eb5a47c7dc66437697e55010c5dc0c44",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "‚Äã",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 170500096/? [00:20&lt;00:00, 31060401.19it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9db55adce6c3462bbc8d62f4cefc69fa"
          }
        },
        "0606fcb7ac384b1cb5054b5389775d9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9ed7ce916d334d7aa427fae1c016d0ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "eb5a47c7dc66437697e55010c5dc0c44": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9db55adce6c3462bbc8d62f4cefc69fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Steve-YJ/Assignment_Standalone_DL/blob/master/Lab7_Basic_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ob3z48HLbj3s",
        "colab_type": "text"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K3DT_zIVblB4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import argparse\n",
        "import numpy as np\n",
        "import time\n",
        "from copy import deepcopy # Add Deepcopy for args"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PE5zxkapblMI",
        "colab_type": "text"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0bahXTR-cBgA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "c38fc0d3621d42edb002754424cd244f",
            "8406ddf5dcc3439b91d371bc8f79dbd4",
            "bf3d7c4abe1e426ba0b85ab7cb3a7d04",
            "063c1529d6f2446c9d3d1af9c55b0c9e",
            "0606fcb7ac384b1cb5054b5389775d9c",
            "9ed7ce916d334d7aa427fae1c016d0ac",
            "eb5a47c7dc66437697e55010c5dc0c44",
            "9db55adce6c3462bbc8d62f4cefc69fa"
          ]
        },
        "outputId": "5129cfbe-279b-4db1-a86d-333dabc8565a"
      },
      "source": [
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
        "                                        download=True, transform=transform)\n",
        "trainset, valset = torch.utils.data.random_split(trainset, [40000, 10000])\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,\n",
        "                                       download=True, transform=transform)\n",
        "partition = {'train': trainset, 'val':valset, 'test':testset}"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c38fc0d3621d42edb002754424cd244f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7QgCRS7bnA6",
        "colab_type": "text"
      },
      "source": [
        "# Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK21ySdvb2r9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MLP(nn.Module):  # nn.Module ÏÉÅÏÜç\n",
        "    def __init__(self, in_dim, out_dim, hid_dim, n_layer, act, dropout, use_bn, use_xavier):\n",
        "        super(MLP, self).__init__()\n",
        "        self.in_dim = in_dim\n",
        "        self.out_dim = out_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layer = n_layer\n",
        "        self.act = act\n",
        "        self.dropout = dropout\n",
        "        self.use_bn = use_bn\n",
        "        self.use_xavier = use_xavier\n",
        "        \n",
        "        # ====== Create Linear Layers ====== #\n",
        "        self.fc1 = nn.Linear(self.in_dim, self.hid_dim)\n",
        "        \n",
        "        self.linears = nn.ModuleList()\n",
        "        self.bns = nn.ModuleList()\n",
        "        for i in range(self.n_layer-1):\n",
        "            self.linears.append(nn.Linear(self.hid_dim, self.hid_dim))\n",
        "            if self.use_bn:\n",
        "                self.bns.append(nn.BatchNorm1d(self.hid_dim))\n",
        "                \n",
        "        self.fc2 = nn.Linear(self.hid_dim, self.out_dim)\n",
        "        \n",
        "        # ====== Create Activation Function ====== #\n",
        "        if self.act == 'relu':\n",
        "            self.act = nn.ReLU()\n",
        "        elif self.act == 'tanh':\n",
        "            self.act == nn.Tanh()\n",
        "        elif self.act == 'sigmoid':\n",
        "            self.act = nn.Sigmoid()\n",
        "        else:\n",
        "            raise ValueError('no valid activation function selected!')\n",
        "        \n",
        "        # ====== Create Regularization Layer ======= #\n",
        "        self.dropout = nn.Dropout(self.dropout)\n",
        "        if self.use_xavier:\n",
        "            self.xavier_init()\n",
        "          \n",
        "    def forward(self, x):\n",
        "        x = self.act(self.fc1(x))\n",
        "        for i in range(len(self.linears)):\n",
        "            x = self.act(self.linears[i](x))\n",
        "            x = self.bns[i](x)\n",
        "            x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "    def xavier_init(self):\n",
        "        for linear in self.linears:\n",
        "            nn.init.xavier_normal_(linear.weight)\n",
        "            linear.bias.data.fill_(0.01)\n",
        "            \n",
        "net = MLP(3072, 10, 100, 4, 'relu', 0.1, True, True) # Testing Model Construction"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIbI78xveD2c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        # Í∏∞Î≥∏Ï†ÅÏù∏ CNN Íµ¨Ï°∞Î•º ÎßåÎì§Ïñ¥Î≥¥Ïûê!\n",
        "        self.conv1 = nn.Conv2d(in_channels = 3,\n",
        "                               out_channels = 64,\n",
        "                               kernel_size = 3,\n",
        "                               stride = 1,\n",
        "                               padding=1)\n",
        "        self.act = nn.ReLU()\n",
        "        # Add MaxPool2D\n",
        "        self.maxpool1 = nn.MaxPool2d(kernel_size=2,\n",
        "                                     stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=64,\n",
        "                               out_channels=256,\n",
        "                               kernel_size=5,\n",
        "                               stride=1,\n",
        "                               padding=2)\n",
        "        # Add FC Layer\n",
        "        self.fc = nn.Linear(65536, 10)  # 10-way Classification\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.act(x)\n",
        "        x = self.maxpool1(x)  # [N, 256, 32, 32] -> [N, 256, 16, 16]\n",
        "        x = x.view(x.size(0), -1)   # batch_size DimensionÏùÄ Ïú†ÏßÄÏãúÏºúÏ§ÄÎã§\n",
        "                                    # x.size(0): N(batch_num)\n",
        "        x = self.fc(x)\n",
        "        return x   "
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Klt-XpCogRkB",
        "colab_type": "text"
      },
      "source": [
        "üìå <code>Ï∞®ÏõêÏàòÎ•º ÌôïÏù∏Ìï¥Ï£ºÎäî Í≤É</code>Ïù¥ ÌïÑÏöîÌïòÎã§!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fdvHx04QgfWk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8b72b2df-59e6-467b-cd07-4609f865d373"
      },
      "source": [
        "def dim_check():  # dimension check function\n",
        "    net = CNN()\n",
        "    x = torch.randn(20, 3, 32, 32)\n",
        "    return net(x).shape  # Expect (20, 10)\n",
        "\n",
        "dim_check()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20, 10])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Vkl79ngb2yo",
        "colab_type": "text"
      },
      "source": [
        "# Train, Validation, Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vXwcZUvb69l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(net, partition, optimizer, criterion, args):\n",
        "    trainloader = torch.utils.data.DataLoader(partition['train'], \n",
        "                                              batch_size=args.train_batch_size, \n",
        "                                              shuffle=True, num_workers=2)\n",
        "    net.train()  # train mode\n",
        "    optimizer.zero_grad()  # optimizer Ï¥àÍ∏∞Ìôî\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    train_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "        # print('input dimension: ', inputs.shape)\n",
        "        # raise RuntimeError\n",
        "        # inputs = inputs.view(-1, 3072)\n",
        "        inputs = inputs.cuda()\n",
        "        labels = labels.cuda()\n",
        "        outputs = net(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    train_loss = train_loss / len(trainloader)\n",
        "    train_acc = 100 * correct / total\n",
        "    return net, train_loss, train_acc\n"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Nv4VaIFb7DS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate(net, partition, criterion, args):\n",
        "    valloader = torch.utils.data.DataLoader(partition['val'], \n",
        "                                            batch_size=args.test_batch_size, \n",
        "                                            shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    val_loss = 0 \n",
        "    with torch.no_grad():\n",
        "        for data in valloader:\n",
        "            images, labels = data\n",
        "            # images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "            outputs = net(images)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            \n",
        "            val_loss += loss.item()\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        val_loss = val_loss / len(valloader)\n",
        "        val_acc = 100 * correct / total\n",
        "    return val_loss, val_acc"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "codal0N7cPFR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(net, partition, args):\n",
        "    testloader = torch.utils.data.DataLoader(partition['test'], \n",
        "                                             batch_size=args.test_batch_size, \n",
        "                                             shuffle=False, num_workers=2)\n",
        "    net.eval()\n",
        "    \n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            # images = images.view(-1, 3072)\n",
        "            images = images.cuda()\n",
        "            labels = labels.cuda()\n",
        "\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        test_acc = 100 * correct / total\n",
        "    return test_acc"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dn_d10alcRpd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def experiment(partition, args):\n",
        "\n",
        "    net = CNN()\n",
        "    net.cuda()\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    if args.optim == 'SGD':\n",
        "        optimizer = optim.SGD(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'RMSprop':\n",
        "        optimizer = optim.RMSprop(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    elif args.optim == 'Adam':\n",
        "        optimizer = optim.Adam(net.parameters(), lr=args.lr, weight_decay=args.l2)\n",
        "    else:\n",
        "        raise ValueError('In-valid optimizer choice')\n",
        "    \n",
        "    # ===== List for epoch-wise data ====== #\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "    # ================= Training ==================== #\n",
        "        \n",
        "    for epoch in range(args.epoch):  # loop over the dataset multiple times\n",
        "        ts = time.time()\n",
        "        net, train_loss, train_acc = train(net, partition, optimizer, criterion, args)\n",
        "        val_loss, val_acc = validate(net, partition, criterion, args)\n",
        "        te = time.time()\n",
        "        \n",
        "        # ====== Add Epoch Data ====== #\n",
        "        train_losses.append(train_loss)\n",
        "        val_losses.append(val_loss)\n",
        "        train_accs.append(train_acc)\n",
        "        val_accs.append(val_acc)\n",
        "        # ============================ #\n",
        "        \n",
        "        print('Epoch {}, Acc(train/val): {:2.2f}/{:2.2f}, Loss(train/val) {:2.2f}/{:2.2f}. Took {:2.2f} sec'.format(epoch, train_acc, val_acc, train_loss, val_loss, te-ts))\n",
        "        \n",
        "    test_acc = test(net, partition, args)    \n",
        "    \n",
        "    # ======= Add Result to Dictionary ======= #\n",
        "    # Ï†ÑÏ≤¥ Í≤∞Í≥ºÍ∞íÏùÑ Ï†ÄÏû•ÌïòÎäî Results Dic \n",
        "    # Q. Ïôú ÎïåÎ¨∏Ïù∏ÏßÑ Î™®Î•¥Í≤†ÏßÄÎßå train_accsÏôÄ train_accÍ∞Ä Î∂ÑÎ¶¨ÎêòÏñ¥ÏûàÎÑ§?!\n",
        "        # train_accÏôÄ val_accÏùÄ ÏóÜÏñ¥ÎèÑ ÎêòÎäîÍ±∞ ÏïÑÎÉê?\n",
        "    result = {}\n",
        "    result['train_losses'] = train_losses\n",
        "    result['val_losses'] = val_losses\n",
        "    result['train_accs'] = train_accs\n",
        "    result['val_accs'] = val_accs\n",
        "    # result['train_acc'] = train_acc\n",
        "    # result['val_acc'] = val_acc\n",
        "    result['test_acc'] = test_acc\n",
        "    return vars(args), result  # vars(args)Î•º Ìï¥Ï£ºÎ©¥ argsÎ•º dicÎ°ú Ï†ÄÏû•Ìï¥Ï§ÄÎã§\n",
        "    # ===================================== #"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9hujgiOcUyT",
        "colab_type": "text"
      },
      "source": [
        "# Manage Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXYBtRc-b5KL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import hashlib\n",
        "import json\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pandas as pd\n",
        "\n",
        "def save_exp_result(setting, result):  # setting: args\n",
        "                                       # result : train_losses, val_losses, train_accs, val_accs,  \n",
        "    exp_name = setting['exp_name']\n",
        "    del setting['epoch']               # settingÏóêÏÑú 'epoch'Í≥º 'test_batch_size'Î•º Ï†úÍ±∞Ìï¥Ï§ÄÎã§Í≥†...\n",
        "    del setting['test_batch_size']     # Ïôú ÎñºÎäîÍ±∞ÏßÄ?\n",
        "\n",
        "    hash_key = hashlib.sha1(str(setting).encode()).hexdigest()[:6]  # settingÏóê Îî∞Îùº Îã§Î•∏ ÌååÏùºÎ™ÖÏùÑ Í∞ñÎèÑÎ°ù ÎßåÎì§Ïñ¥Ï§ÄÎã§\n",
        "    filename = './results/{}-{}.json'.format(exp_name, hash_key)\n",
        "    result.update(setting)  # Í≤∞Í≥º dictionaryÏóê settingÍ∞íÏùÑ ÎçîÌï¥Ï§ÄÎã§ => Dic\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(result, f)  # JSON Ìè¨Î©ßÏúºÎ°ú dictionary Í∞í Ï†ÄÏû•\n",
        "\n",
        "    \n",
        "def load_exp_result(exp_name):\n",
        "    dir_path = './results'\n",
        "    filenames = [f for f in listdir(dir_path) if isfile(join(dir_path, f)) if '.json' in f]  # ./resutsÏóê Ï†ÄÏû•ÎêòÏñ¥ÏûàÎäî fileÎì§ÏùÑ Î¶¨Ïä§Ìä∏ ÌòïÌÉúÎ°ú Ï†ÄÏû•\n",
        "    list_result = []\n",
        "    for filename in filenames:\n",
        "        if exp_name in filename:\n",
        "            # print(exp_name)\n",
        "            with open(join(dir_path, filename), 'r') as infile:\n",
        "                results = json.load(infile)\n",
        "                list_result.append(results)  # DicÏùÑ listÌòïÌÉúÎ°ú Ï†ÄÏû•\n",
        "    df = pd.DataFrame(list_result) # .drop(columns=[])\n",
        "    return df"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzDemSKOcbkf",
        "colab_type": "text"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pce478VkcaAA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "outputId": "ecabd498-7d1b-4019-f9c5-caf0e125e294"
      },
      "source": [
        "# ====== Random Seed Initialization ====== #\n",
        "seed = 123  # ÎûúÎç§ ÏãúÎìúÍ∞í Ï¥àÍ∏∞Ìôî\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "args = parser.parse_args(\"\")\n",
        "args.exp_name = \"exp1_n_layer_hid_dim\"\n",
        "\n",
        "# ====== Model Capacity ====== #\n",
        "args.out_dim = 10\n",
        "# args.hid_dim = 100\n",
        "args.act = 'relu'\n",
        "\n",
        "# ====== Regularization ======= #\n",
        "args.l2 = 0.00001\n",
        "\n",
        "# ====== Optimizer & Training ====== #\n",
        "args.optim = 'RMSprop' #'RMSprop' #SGD, RMSprop, ADAM...\n",
        "args.lr = 0.0015\n",
        "args.epoch = 10\n",
        "\n",
        "args.train_batch_size = 256\n",
        "args.test_batch_size = 1024\n",
        "\n",
        "# ====== Experiment Variable ====== #\n",
        "name_var1 = 'lr'\n",
        "name_var2 = 'l2'\n",
        "list_var1 = [0.0001]\n",
        "list_var2 = [0.00001]\n",
        "\n",
        "\n",
        "for var1 in list_var1:\n",
        "    for var2 in list_var2:\n",
        "        # setattr ??: name_var1('n_layer')Î•º 1, 2, 3ÏúºÎ°ú Î∞îÍøîÏ§ÄÎã§\n",
        "        # setattr = args.name_var1 = var1\n",
        "        setattr(args, name_var1, var1)\n",
        "        setattr(args, name_var2, var2)\n",
        "        print(args)\n",
        "                \n",
        "        setting, result = experiment(partition, deepcopy(args))\n",
        "        save_exp_result(setting, result)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(act='relu', epoch=10, exp_name='exp1_n_layer_hid_dim', l2=1e-05, lr=0.0001, optim='RMSprop', out_dim=10, test_batch_size=1024, train_batch_size=256)\n",
            "Epoch 0, Acc(train/val): 34.01/44.70, Loss(train/val) 1.83/1.54. Took 6.89 sec\n",
            "Epoch 1, Acc(train/val): 48.87/52.45, Loss(train/val) 1.43/1.36. Took 6.78 sec\n",
            "Epoch 2, Acc(train/val): 53.46/55.15, Loss(train/val) 1.31/1.29. Took 6.66 sec\n",
            "Epoch 3, Acc(train/val): 56.98/57.23, Loss(train/val) 1.22/1.22. Took 6.89 sec\n",
            "Epoch 4, Acc(train/val): 59.01/57.84, Loss(train/val) 1.16/1.21. Took 6.90 sec\n",
            "Epoch 5, Acc(train/val): 60.95/59.45, Loss(train/val) 1.12/1.17. Took 6.78 sec\n",
            "Epoch 6, Acc(train/val): 62.46/60.07, Loss(train/val) 1.06/1.15. Took 7.02 sec\n",
            "Epoch 7, Acc(train/val): 63.60/61.17, Loss(train/val) 1.04/1.12. Took 6.84 sec\n",
            "Epoch 8, Acc(train/val): 65.50/61.86, Loss(train/val) 0.98/1.10. Took 6.87 sec\n",
            "Epoch 9, Acc(train/val): 66.08/62.97, Loss(train/val) 0.97/1.08. Took 6.85 sec\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-c8e1a8de8a5c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m         \u001b[0msave_exp_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-24-fb2cf709573e>\u001b[0m in \u001b[0;36msave_exp_result\u001b[0;34m(setting, result)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./results/{}-{}.json'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhash_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msetting\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Í≤∞Í≥º dictionaryÏóê settingÍ∞íÏùÑ ÎçîÌï¥Ï§ÄÎã§ => Dic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# JSON Ìè¨Î©ßÏúºÎ°ú dictionary Í∞í Ï†ÄÏû•\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './results/exp1_n_layer_hid_dim-6bb6e9.json'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSCNXhixi88I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}