{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Pytorch-Tutorial] Finetuning Torchvision Models.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPc+/rtlvb9C9PTuJngn9fz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Steve-YJ/Assignment_Standalone_DL/blob/master/%5BPytorch_Tutorial%5D_Finetuning_Torchvision_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtDKJudyNaG6"
      },
      "source": [
        "# Fine-Tuning Torchvision Models\n",
        "- how to finetune and feature extract the torchvision models, all of which have been pretrained on the 1000-class imagenet dataset\n",
        "- how to work with several modern CNN architectures\n",
        "- build an intuition for fine-tunning any PyTorch model\n",
        "- Since each model architecture is different, there is no boilerplate fine-tunning code that will work in all scenarios\n",
        "- Rather, the researcher must look at the existing architecture and make custom adjustments for each model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3AX-ZOmfNjAn"
      },
      "source": [
        "## In this document\n",
        "\n",
        "- we'll perform two types of transfer learning\n",
        "    - fine-tuning\n",
        "    - feature extraction\n",
        "- In fine-tuning\n",
        "    - we start with a pre-trained model and update all of the model's parameters for our new task, in essence retraining the whole model\n",
        "- In feature extraction\n",
        "    - we start with a pre-trained model and only update the final layer weights from which we derive predictions.\n",
        "    - It is called feature extraction because we use the pre-trained CNN as a fixed feature-extractor, and only change the output layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J3MdSHVYPVPc"
      },
      "source": [
        "✅ Fine-Tune: <br>\n",
        "* re-training the whole model<br>\n",
        "* update all of the model's parameters for our new task\n",
        "\n",
        "✅ Feature Extraction: <br>\n",
        "* start with pre-trained model and only update the final layer weights from which derive predictions<br>\n",
        "* It is called feature extraction because we use the pre-trained CNN as a fixed feature-extrator, only change the output layer\n",
        "\n",
        "In general both transfer learning methods follow the same few steps:\n",
        "\n",
        "- Initialize the pre-trained model\n",
        "- Reshape the final layer(s) to have the same number of outputs as the number of classes in the new dataset\n",
        "- Define for the optimization algorithm which parameters we want to update during training\n",
        "- Run the training step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAkZiZ-2RBxs",
        "outputId": "1746e038-d29d-4f29-eac5-1e6974b749a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "\"\"\"\n",
        "    Just do it!\n",
        "\"\"\""
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n    Just do it!\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZhdAfPHN_90"
      },
      "source": [
        "## Reference\n",
        "* <a ref='https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html'>Pytorch.org - 'Fine-Tuning Torchvision Models'</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11uSk68OSOhx"
      },
      "source": [
        "# Drive Mount\n",
        "* for using google drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VTISdD9sSO7V",
        "outputId": "5fcd1e90-f566-4443-eab2-7d3ea86b7675",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "# mount drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o-N3L1LmSOI6",
        "outputId": "3f75ae90-cec0-4e91-f5e8-e3d5d3ed0c4a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OT8BImZXSvt4",
        "outputId": "2eb89412-cbf0-4e7a-a316-1c708d0bc6ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd drive/My\\ Drive/TIL(Today-I-learned)/[Pytorch-Tutorial]"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TIL(Today-I-learned)/[Pytorch-Tutorial]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_G7r5FAdS_sD",
        "outputId": "6ed3001d-685e-4631-8512-5162dcd7bf10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "! ls"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " hymenoptera_data  '[Pytorch-Tutorial] Finetuning Torchvision Models.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQ6vV2grRg9I"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OAp5bHWcjVS",
        "outputId": "76162772-9c51-40e1-9819-016d892b20af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from __future__ import print_function\n",
        "from __future__ import division\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, models, transforms\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import os\n",
        "import copy\n",
        "\n",
        "print(\"Pytorch Version: \", torch.__version__)\n",
        "print(\"Torchvision Version: \", torchvision.__version__)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Pytorch Version:  1.7.0+cu101\n",
            "Torchvision Version:  0.8.1+cu101\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WsXqHNOLdsYU"
      },
      "source": [
        "## Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZZfn-ejSJKB"
      },
      "source": [
        "* Dataset\n",
        "    * We will use the hymenoptera_data dataset\n",
        "    * This dataset contains two classes, bees and ants, and is structured such that we can use the ImageFolder dataset,\n",
        "\n",
        "* Download the data and set the <code>data_dir</code> input to the root directory of the dataset. \n",
        "* The <code>model_name</code> input is the name of the model you wish to use and must be selected from this list:\n",
        "    \n",
        "        > [resnet, alexnet, vgg, squeezenet, densnet, inception]\n",
        "\n",
        "\n",
        "* If you want to find more, click <a>this</a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y3SX4763UNWa"
      },
      "source": [
        "✅ Summary<Br>\n",
        "* <code>data_dir</code>\n",
        "* <code>model_name</code>\n",
        "* <code>num_classes</code>\n",
        "* <code>batch_size</code>\n",
        "* <code>num_epochs</code>\n",
        "* <code>feature_extract</code> is a boolean that defines if we are fine-tuning or feature extracting\n",
        "    * if <code>feature_extract = False</code>: the model is fine-tuned and all model parameters are updated\n",
        "    * if <code>feature_extract = True</code>: only last layer parameters are updatated, the others remain fixed"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ai4Sv8LId5zD"
      },
      "source": [
        "# Top level data directory, Here we assume the format of the directory conforms to the ImageFolder structure\n",
        "data_dir = \"./data/hymenoptera_data\"\n",
        "\n",
        "# Models to choose from [resnet, alexnet, vgg, squeezenet, densenet, inception]\n",
        "model_name = \"squeezenet\"\n",
        "\n",
        "# Number of classes in the dataset\n",
        "num_classes = 2\n",
        "\n",
        "# Batch size for training (chane depending on how much memory you have)\n",
        "batch_size = 8\n",
        "\n",
        "# Number of epochs to train for\n",
        "num_epochs = 15\n",
        "\n",
        "# Flag for feature extracting. When False, we finetune the whole model,\n",
        "#   when True we only update the reshaped layer params\n",
        "# Feature Extracting 할거니 아니면 전체 모델을 fine-tune 할거니\n",
        "\n",
        "feature_extract = True"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCm1OhCiWPNo"
      },
      "source": [
        "## Helper Functions\n",
        "* Q. Wht is Helper Functions?\n",
        "    * Before we write the code for adjusting the models, lets define a few helper functions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uvO3iUpFWQfv"
      },
      "source": [
        "## Model Training and Vlidation Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVMfjAWxWQc7"
      },
      "source": [
        "* The <code>train_model</code> function handles the training and validation of a given model.\n",
        "* As input, it takes a PyTorch model, a dictionary of dataloaders, a loss functions, as optimizer, a specified number of epochs to train and validate for, and a boolean flag for when the model is an Inception model.\n",
        "* ❓ The <code>is_inception</code> flag is used to accomodate the Inception v3 model, as that architecture uses an auxiliary output and he overall model loss respects both the auxiliary output and the final output, as described here. \n",
        "* ❓ The function trains for the specified number of epochs and after each epoch runs a full validation step. \n",
        "* ❓ It also keeps track of the best performing model (in terms of validation accuracy), and at the end of training returns the best performing model. After each epoch, the training and validation accuracies are printed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ylyDsNnlY10S"
      },
      "source": [
        "# Define train_model function\n",
        "\n",
        "def train_model(model, dataloaders, criterion, optimizer, num_epochs, is_inception=False):\n",
        "    since = time.time()\n",
        "\n",
        "    val_acc_history = []\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs-1))\n",
        "        print('-'*10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)  # cuda\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    # Get model outputs and calculate loss\n",
        "                    # Special case for inception because in training it has an auxiliary output.\n",
        "                    # In train mode, we calculate the loss by summing the final output and the auxiliary output\n",
        "                    # but in testing we only consider the final output\n",
        "\n",
        "                    if is_inception and phase == 'train':\n",
        "                        # From https://discuss.pytorch.org/t/how-to-optimize-inception-model-with-auxiliary-classifiers/7958\n",
        "                        outputs, aux_outputs = model(inputs)\n",
        "                        loss1 = criterion(outputs, labels)\n",
        "                        loss2 = criterion(aux_outputs, labels)\n",
        "                        loss = loss1 + 0.4*loss2  # How funny is it\n",
        "\n",
        "                    else:\n",
        "                        outputs = model(inputs)  # If not Inception Model, outputs is just model's output\n",
        "                        loss = criterion(outputs, 1)\n",
        "                    \n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                    # backward + optimize only if in training phase \n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            # Actually, the loss of an epoch is usually defined as the average of the loss of batches in that epoch. \n",
        "            # So you can accumulate the loss values during an epoch and at the end divide it by the number of batches in the epoch:\n",
        "            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "            if phase == 'val':\n",
        "                val_acc_history.append(epoch_acc)\n",
        "\n",
        "        print()\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed//60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, val_acc_history"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zGlPSbscWTzE"
      },
      "source": [
        "## Initialize and Reshape the Networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WOdtGALAWLk9"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7lKmr0kWYKO"
      },
      "source": [
        "## ResNet\n",
        "* Hi :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8C3sQdg0WbvZ"
      },
      "source": [
        "## Alexnet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvJINPnNWf-1"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4x5Gd-TBWgA6"
      },
      "source": [
        "## VGG"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUIBsvdEWhU8"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TXGnaofPWhei"
      },
      "source": [
        "## Squeezenet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SgCcdGi0Whg6"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5MK8YCpFWmOj"
      },
      "source": [
        "## Inception v3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u7Y3d7E-WoG2"
      },
      "source": [
        "# Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8C1i_iZKWqly"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUgznEgZWstF"
      },
      "source": [
        "# Run Training and Validation Step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Lq_O1c_WvkT"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aqgrebhRWvuw"
      },
      "source": [
        "# Comparison with Model Trained from Scratch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "leMTdLgYW0nS"
      },
      "source": [
        ""
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aDEQLuAMW2NX"
      },
      "source": [
        "# Final Thoughts and Where to Go Next "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePe_GeaEW6DZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}